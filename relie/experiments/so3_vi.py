"""
Simple example of Variational Inference.

We have known data X_0 (in a Linear G space).
We observe some single X that is generated by G_0(X_0) by unknown G_0.
We have given likelihood model:
p(X|G) = N(X | G(X_0), sigma)

Now we do VI to infer p(G|X), so we optimise the ELBO:
for variational family q(G) and uniform prior:

E_{G \sim q) \log p(X|G) - log q(G)

As variational family, we use a Gaussian on the algebra.
"""
import torch
import torch.nn as nn
from torch.distributions import Normal
from relie.flow import LocalDiffeoTransformedDistribution as LTDT
from relie.lie_distr import SO3Prior, SO3ExpTransform, SO3MultiplyTransform
from relie.utils.so3_tools import so3_log, so3_vee, so3_exp

torch.manual_seed(0)
x_zero = torch.randn(100, 3, dtype=torch.double)
g_zero = SO3Prior(dtype=torch.double).sample((1,))[0]
g_zero_alg = so3_vee(so3_log(g_zero))
x = (g_zero @ x_zero.t()).t()


def prediction_loss_fn(g, x, x_zero):
    """
    Prediction loss = -log p(X|G)
    Allows for batching in G.
    :param g: Group shape (..., 3, 3)
    :param x: (n, 3)
    :param x_zero: (n, 3)
    :return: (...,)
    """
    d = (x.t() - (g[..., None, :] @ x_zero.t())[..., :, 0, :])
    return d.pow(2).sum(-1).sum(-1)


class Model(nn.Module):
    def __init__(self):
        super().__init__()

        self.loc = nn.Parameter(torch.zeros(3).double())
        self.pre_scale = nn.Parameter(torch.ones(3).double())
        self.transform = SO3ExpTransform()

    def rsample(self, size):
        alg_distr = Normal(torch.zeros(3).double(),
                           F.softplus(self.pre_scale))
        distr = LDTD(alg_distr, self.transform)
        distr = LDTD(distr, SO3MultiplyTransform(so3_exp(self.loc)))
                           
        return distr.rsample(size)
    
    def log_prob(self, x):
        alg_distr = Normal(torch.zeros(3).double(),
                           F.softplus(self.pre_scale))
        distr = LDTD(alg_distr, SO3ExpTransform())
        distr = LDTD(distr, SO3MultiplyTransform(so3_exp(self.loc)))
        
        return distr.log_prob(x) 
        
    def forward(self, x, x_zero):
        #alg_distr = Normal(self.loc * 1, self.pre_scale * 1)
        alg_distr = Normal(torch.zeros(3).double(),
                           F.softplus(self.pre_scale))
        distr = LDTD(alg_distr, self.transform)
        distr = LDTD(distr, SO3MultiplyTransform(so3_exp(self.loc)))
        #distr = LDTD(alg_distr, self.transform)
        g = distr.rsample((64,))

        prediction_loss = prediction_loss_fn(g, x, x_zero).float()
        entropy = -distr.log_prob(g)
        loss = prediction_loss - entropy
        return loss


model = Model()
optimizer = torch.optim.Adam(model.parameters())

for it in range(10000):
    loss = model(x, x_zero).mean()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if it % 1000 == 0:
        print(f"Loss: {loss.item()}.")
        print(f"Parameters: {torch.cat([model.loc, model.scale]).tolist()}")
        print(f"Target at {g_zero_alg.tolist()}")

