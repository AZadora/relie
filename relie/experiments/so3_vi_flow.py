"""
Simple example of Variational Inference.

We have known data X_0 (in a Linear G space).
We observe some single X that is generated by G_0(X_0) by unknown G_0.
We have given likelihood model:
p(X|G) = N(X | G(X_0), sigma)

Now we do VI to infer p(G|X), so we optimise the ELBO:
for variational family q(G) and uniform prior:

E_{G \sim q) \log p(X|G) - log q(G)

As variational family, we use a Gaussian on the algebra.
"""
import math
from itertools import cycle
import torch
import torch.nn as nn
from torch.distributions import Normal, ComposeTransform
from relie.flow import LocalDiffeoTransformedDistribution as LDTD, PermuteTransform, CouplingTransform, RadialTanhTransform, BatchNormTransform
from relie.lie_distr import SO3Prior, SO3ExpTransform, SO3ExpCompactTransform
from relie.utils.modules import MLP, BatchSqueezeModule, ToTransform
from relie.utils.so3_tools import so3_log, so3_vee


torch.manual_seed(0)
x_zero = torch.randn(100, 3, dtype=torch.double)
g_zero = SO3Prior(dtype=torch.double).sample((1,))[0]
g_zero_alg = so3_vee(so3_log(g_zero))
x = (g_zero @ x_zero.t()).t()


def prediction_loss_fn(g, x, x_zero):
    """
    Prediction loss = -log p(X|G)
    Allows for batching in G.
    :param g: Group shape (..., 3, 3)
    :param x: (n, 3)
    :param x_zero: (n, 3)
    :return: (...,)
    """
    d = (x.t() - (g[..., None, :] @ x_zero.t())[..., :, 0, :])
    return d.pow(2).sum(-1).sum(-1)


class Flow(nn.Module):
    def __init__(self, d, n_layers, batch_norm=True, net_layers=3):
        super().__init__()
        self.d = d
        self.d_residue = 1
        self.d_transform = d - self.d_residue

        self.nets = nn.ModuleList([
            MLP(self.d_residue,
                2 * self.d_transform,
                50,
                net_layers,
                batch_norm=False) for _ in range(n_layers)
        ])
        self._set_params()
        r = list(range(3))
        self.permutations = [r[i:] + r[:i] for i in range(3)]

        if batch_norm:
            self.batch_norms = nn.ModuleList([
                nn.BatchNorm1d(d) for _ in range(n_layers)])
        else:
            self.batch_norms = [None] * n_layers

    def forward(self):
        transforms = []
        for i, (net, bn, permutation) in enumerate(
                zip(self.nets, self.batch_norms, cycle(self.permutations))):
            if bn is not None:
                transforms.append(BatchNormTransform(bn))
            transforms.extend([
                CouplingTransform(self.d_residue, BatchSqueezeModule(net)),
                PermuteTransform(permutation),
            ])
        return ComposeTransform(transforms)

    def _set_params(self):
        """
        Initialize coupling layers to be identity.
        """
        for net in self.nets:
            last_module = list(net.modules())[-1]
            last_module.weight.data = torch.zeros_like(last_module.weight)
            last_module.bias.data = torch.zeros_like(last_module.bias)


class FlowDistr(nn.Module):
    def __init__(self, flow, algebra_support_radius=math.pi * 1.6):
        super().__init__()
        self.flow = flow
        self.register_buffer('prior_loc', torch.zeros(3))
        self.register_buffer('prior_scale', torch.ones(3))
        self.intermediate_transform = ComposeTransform([
            RadialTanhTransform(algebra_support_radius),
            ToTransform(dict(dtype=torch.float32), dict(dtype=torch.float64))
        ])
        self.algebra_support_radius = algebra_support_radius

    def transforms(self):
        transforms = [
            self.flow(),
            self.intermediate_transform,
            SO3ExpCompactTransform(self.algebra_support_radius),
        ]
        return transforms

    def forward(self):
        prior = Normal(self.prior_loc, self.prior_scale)
        return LDTD(prior, self.transforms())


class Model(nn.Module):
    def __init__(self, flow_distr):
        super().__init__()
        self.flow_distr = flow_distr

    def forward(self, x, x_zero):
        distr = self.flow_distr()
        g = distr.rsample((64,))

        prediction_loss = prediction_loss_fn(g, x, x_zero).float()
        entropy = -distr.log_prob(g)
        loss = prediction_loss - entropy
        return loss


flow = Flow(3, 4)
flow_distr = FlowDistr(flow, math.pi * 1.1)
model = Model(flow_distr)
optimizer = torch.optim.Adam(model.parameters(), lr=1E-4)

for it in range(10000):
    loss = model(x, x_zero).mean()
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if it % 1000 == 0:
        print(f"Loss: {loss.item()}.")
        # print(f"Parameters: {torch.cat([model.loc, model.scale]).tolist()}")
        # print(f"Target at {g_zero_alg.tolist()}")

